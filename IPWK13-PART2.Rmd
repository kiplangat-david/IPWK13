---
title: "IPWK13 - Kira Plastinina Project"
author: "KIPLANGAT-DAVID"
date: "January 27, 2022"
output: html_document
---

##RESEARCH QUESTION##

Kira Plastinina is a Russian brand that is sold through a defunct chain of retail stores in Russia, Ukraine, Kazakhstan, Belarus, China, Philippines, and Armenia. The brand's Sales and Marketing team would like to understand their customer's behavior from data that they have collected over the past year. More specifically, they would like to learn the characteristics of customer groups.

##PROBLEM STATEMENT##

Most website business attracts many visitors. Anyone who land on a business website has some sort of interest in what one has to offer. However, not everyone would visit a site to buy. Some users might be interested in only gaining the information, and not making a purchase. The result will be a loss lead and sales if the conversion rates are low.
The objective of this analysis therefore is to provide a reliable and feasible recommendation algorithm to understand their customer's behavior from data that they have collected over the past year. More specifically, they would like to learn the characteristics of customer groups.. The target value is the binary 'FALSE' or 'TRUE' regarding the website visitors' decision to buy. The plan is to use clustering and maybe classification techniques to be able to make predictions about shoppers' intentions.

##DATA##

The dataset we are using consists of 10 numerical and 8 categorical attributes.

**Administrative, Administrative Duration, Informational, Informational Duration, Product Related and Product Related Duration** represent the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories. The values of these features are derived from the URL information of the pages visited by the user and updated in real time when a user takes an action, e.g. moving from one page to another.

The **Bounce Rate, Exit Rate and Page Value** features represent the metrics measured by "Google Analytics" for each page in the e-commerce site.

**Bounce Rate** - feature for a web page refers to the percentage of visitors who enter the site from that page and then leave ("bounce") without triggering any other requests to the analytics server during that session. This is the number of single-page visits by visitors of the website.

**Exit Rate** - feature for a specific web page is calculated as for all pageviews to the page, the percentage that were the last in the session. This is the number of exits from the website.

**Page Value** - feature represents the average value for a web page that a user visited before completing an e-commerce transaction. It tells you which specific pages of the site offer the most value. For instance, a product page for an Ecommerce site will usually have a higher page value than a resource page.

**Special Day** - feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother's Day, Valentine's Day) in which the sessions are more likely to be finalized with transaction. The value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. For example, for Valentina's day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8.

The dataset also includes **operating system, browser, region, traffic type, visitor type** as *returning*,*other* or *new visitor*, a Boolean value indicating whether the date of the visit is weekend, and month of the year.


**Revenue** - has the client purchased a product on the website? (binary: 'TRUE', 'FALSE')

##LIBRARIES##


```{r}
## Libraries used
#install.packages('models')

library(ggplot2)
library(tidyverse)
library(gmodels)
library(ggmosaic)
library(corrplot)
library(caret)
library(rpart)
library(rpart.plot)
library(fpc)
library(data.table)
library(plyr)
library(dplyr)
library(cluster)
library(factoextra) #To create a beautiful graph of the clusters    generated with the kmeans() function
library(NbClust)
library(flextable)
options(warn=-1) #turn off warnings

```

##LOADING DATA##
```{r}
df <- read.csv('http://bit.ly/EcommerceCustomersDataset')

#preview top 5 records
head(df)

```

##Check class type##

```{r}
# Checking the type of the dataset
class(df)
```

##Number of rows and columns##

```{r}
# Checking the shape of the dataframe
dim(df)
```

- Our dataset has 12330 rows and 18 columns


##DATA CLEANING##

##Missing Values##
```{r}
# Checking for missing values
colSums(is.na(df))

```

- Our dataset contain missing values. The missing values are very few compared to the records we have and so dropping them will not cost us anything.

```{r}
# Drop missing values
df = na.omit(df)

```


##Duplicates##
```{r}
# Checking for duplicated data
anyDuplicated(df)


```

- We have 159 duplicates. we will also drop them

```{r}
# Dropping duplicates
df = distinct(df)

# Ckecking whether the duplicates have been successfully dropped
anyDuplicated(df)
```

##Make column names uniform##

```{r}
# Cleaning column names, by making them uniform
colnames(df) = tolower(colnames(df))
```

##Data Types##

```{r}
# Checking the datatypes for each column
columns = colnames(df)
for (column in seq(length(colnames(df)))){
    print(columns[column])
    print(class(df[, column]))
    cat('\n')
}# Checking the datatypes for each column
columns = colnames(df)
for (column in seq(length(colnames(df)))){
    print(columns[column])
    print(class(df[, column]))
    cat('\n')
}
```

##Data Structure##

```{r}
str(df)
```

Our dataset has:
- 12199 observations and 18 variables
- 7 integer variables
- 7 numerical variables
- 2 character variables
- 2 logical variables




##EXPLORATORY DATA ANALYSIS##

##Summary Statistics##

```{r}
summary(df)
```

- Summary statistics shows minimum,maximum,mean,median,Q1 & Q3 of every variable

##Check distribution of variables##

##Revenue##
- class label
  -has the client purchased a product on the website? (binary: 'TRUE', 'FALSE')
  
```{r}
CrossTable(df$revenue)
```


```{r}
#check distribution of target variable
colors <- c("tomato", "royalblue")

revenue <- table(df$revenue)

barplot(revenue, col = colors,
        main = "Revenue Distribution")

```

- 15.6% clients purchased a product on website while 84.4% did not

##VisitorType##

```{r}
CrossTable(df$visitortype)
```

```{r}
#check distribution of visitortype
colors <- c("tomato", "royalblue")

visitors <- table(df$visitortype)

barplot(visitors, col = colors,
        main = "Visitor Type")

```

- 85.5% of users represent returning visitors, 13.9% new visitors and 0.007% others

##Weekend##

-date of the visit is weekend? (binary: 'TRUE', 'FALSE')

```{r}
CrossTable(df$weekend)
```


```{r}
#check distribution of weekends
colors <- c("tomato", "royalblue")

weekend <- table(df$weekend)

barplot(weekend, col = colors,
        main = "Weekend")

```

- 76.6% visits were made during weekdays and 23.4% during weekends

##Month##

```{r}
CrossTable(df$month)
```


```{r}

barchart(df$month, 
        main = "Months", srt = 35)

```

-The website was visited most in May(27.3%), November(24.5%) and March(15.2%)


##Month and Revenue##

```{r}
CrossTable(df$month, df$revenue)

```



```{r}
colors <- c("tomato", "royalblue")

revenue <- with(df, table(revenue , month))

barplot(revenue, legend = TRUE, beside = TRUE, col = colors,
        main = "Revenue by Month")

```

- November stands out with 25.5% of the visitors buying out of 2983 visitors
- February had least buyers (1.6%) with 182 total visitors

##VisitorType and Revenue##

```{r}
CrossTable(df$visitortype, df$revenue)
```


```{r}
colors <- c("tomato", "royalblue")

revenue <- with(df, table(revenue , visitortype))

barplot(revenue, legend = TRUE, beside = TRUE, col = colors,
        main = "Revenue by VisitorType")

```

- Large percentage (24.9%) of new visitors made purchases out of 1693 total new visitors.


##Weekend and Revenue##

```{r}
CrossTable(df$weekend, df$revenue)

```

```{r}
colors <- c("tomato", "royalblue")

weekend <- with(df, table(revenue , weekend))

barplot(weekend, legend = TRUE, beside = TRUE, col = colors,
        main = "Weekend")

```
-We see that 76.7% of our visitors visiting on the weekday, a five-day period, with a 15.1% chance of buying something and 23.4% of visitors on weekends, a two-day period, and a 17.5% chance of buying.



##Operating system type and Revenue##
```{r}
CrossTable(df$operatingsystems, df$revenue)

```

```{r}
df %>% 
  ggplot() +
  geom_mosaic(aes(x = product(revenue, operatingsystems), fill = revenue)) +
  xlab("OS Types") 
 

```


-OS type 8 stands out with 22.7% of shoppers buying out of 75 visitors using this. 
-The lowest we see is OS type 6 with only 10.5% yes
-majority of visitors are from OS types 1, 2, and 3.


##Browser Type and Revenue##

```{r}
CrossTable(df$browser, df$revenue)

```

```{r}
df %>% 
  ggplot() +
  geom_mosaic(aes(x = product(revenue, browser), fill = revenue)) +
  xlab("Broswer Types")
 

```
-Browser of type 1 has many users (2426) while browser type 9 has the least (1)
- Browser 12 and 13 show high conversions (30% and 26.6 % respectively) 


##Region and Revenue##

```{r}
CrossTable(df$region, df$revenue)

```

```{r}
df %>% 
  ggplot() +
  geom_mosaic(aes(x = product(revenue, region), fill = revenue)) +
  xlab("Regions") 
  

```

- Very little variation by region ranging from 13.0% to 17.0 %


##Trafic Type and Revenue##

```{r}
CrossTable(df$traffictype, df$revenue)

```

```{r}
df %>% 
  ggplot() +
  geom_mosaic(aes(x = product(revenue, traffictype), fill = revenue)) +
  xlab("Traffic Type") 
  

```

- Lots of variation in revenue with the type of traffic the website is getting.


##DATA PREPROCESSING##

##drop revenue column-(label for this case)##

```{r}
df <- within(df,rm('revenue'))
```

##Encode Categorical Variables##
```{r}
dmy <- dummyVars(" ~ .", data = df, fullRank = T)
df1 <- data.frame(predict(dmy, newdata = df))
glimpse(df1)
```


##Data Scaling##

```{r}
df1 <- scale(df1)

```


##KMEANS CLUSTERING##

K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K. The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarityr

- First, we will find the optimal number of clusters using the elbow method

##Elbow method##

The Elbow method looks at the total within-cluster sum of square (WSS) as a function of the number of clusters.

```{r}
#Elbow Method for finding the optimal number of clusters
set.seed(123)
# Compute and plot wss for k = 2 to k = 15.
k.max <- 15
data <- df1
wss <- sapply(1:k.max, 
              function(k){kmeans(data, k, nstart=25,iter.max = 15 )$tot.withinss})
wss
plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")


```
- we use k=12
we specify nstart = 25. This means that R will try 50 different random starting assignments and then select the best results corresponding to the one with the lowest within cluster variation

```{r}
# Performing clustering with the optimal number of clusters
k_means <- kmeans(df1, 12,nstart = 25)

# Checking the cluster centers of each variable
k_means$centers

```

```{r}
# Previewing the size of observations in each cluster
k_means$size

```


```{r}
## Our cluster centers (means)
k_means$centers
```

```{r}
## Between cluster sum of squares
k_means$betweenss
```

```{r}
## Total cluster sum of squares
k_means$totss
```

```{r}
## Whithin clusters sum of squares
k_means$betweenss / k_means$totss
```
-the quality of the partition is 46.34 %

Suggests the model is not so accurate at prediction.



**HIERARCHICAL CLUSTERING**

```{r}
# Dissimilarity matrix
d <- dist(df1,method = "euclidean")
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete")
hcl
```


```{r}
# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1)
```

- dendrogram is built and every data point finally merges into a single cluster with the height(distance) shown on the y-axis.


##COMPARISON##

##KMEANS CLUSTERING##
K-means clustering is a machine learning clustering technique used to simplify large datasets into smaller and simple datasets. Distinct patterns are evaluated and similar data sets are grouped together. The variable K represents the number of groups in the data. This article evaluates the pros and cons of the K-means clustering algorithm to help you weigh the benefits of using this clustering technique.

 

##Pros:##
1. Simple: It is easy to implement k-means and identify unknown groups of data from complex data sets. The results are presented in an easy and simple manner.

2. Flexible: K-means algorithm can easily adjust to the changes. If there are any problems, adjusting the cluster segment will allow changes to easily occur on the algorithm.
3. Suitable in a large dataset: K-means is suitable for a large number of datasets and it's computed much faster than the smaller dataset. It can also produce higher clusters.

4. Efficient: The algorithm used is good at segmenting the large data set. Its efficiency depends on the shape of the clusters. K-means works well in hyper-spherical clusters.

5. Time complexity: K-means segmentation is linear in the number of data objects thus increasing execution time. It doesn't take more time in classifying similar characteristics in data like hierarchical algorithms.

6. Easy to interpret: The results are easy to interpret. It generates cluster descriptions in a form minimized to ease understanding of the data.

7. Computation cost: Compared to using other clustering methods, a k-means clustering technique is fast and efficient in terms of its computational cost O(K*n*d).

8. Accuracy: K-means analysis improves clustering accuracy and ensures information about a particular problem domain is available. Modification of the k-means algorithm based on this information improves the accuracy of the clusters.



 

##Cons:##

1. NoNo-optimal set of clusters: K-means doesn't allow the development of an optimal set of clusters and for effective results, you should decide on the clusters before.

2. Lacks consistency: K-means clustering gives varying results on different runs of an algorithm. A random choice of cluster patterns yields different clustering results resulting in inconsistency.

3. Uniform effect: It produces clusters with uniform sizes even when the input data has different sizes.
4. Order of values: The way in which data is ordered in building the algorithm affects the final results of the data set.

5. Sensitivity to scale: Changing or rescaling the dataset either through normalization or standardization will completely change the final results.

6. Crash computer: When dealing with a large dataset, conducting a dendrogram technique will crash the computer due to a lot of computational load and Ram limits.
7. Handle numerical data: K-means algorithm can be performed in numerical data only.

8. Operates in assumption: K-means clustering technique assumes that we deal with spherical clusters and each cluster has equal numbers for observations. The spherical assumptions have to be satisfied. The algorithm can't work with clusters of unusual size.
9. Specify K-values: For K-means clustering to be effective, you have to specify the number of clusters (K) at the beginning of the algorithm.

10. Prediction issues: It is difficult to predict the k-values or the number of clusters. It is also difficult to compare the quality of the produced clusters.

##HIERARCHICAL CLUSTERING##

Hierarchical clustering works  by treating each observation as a separate cluster. Then, it repeatedly executes the following two steps: 
(1) identify the two clusters that are closest together, and 
(2) merge the two most similar clusters. 
This iterative process continues until all the clusters are merged together.

##Pros:##

 -We do not need to specify the number of clusters required for the algorithm.
 -Hierarchical clustering outputs a hierarchy, ie a structure that is more informative than the unstructured set of flat clusters returned by k-means.
 -It is also easy to implement.

##Cons:##

 -There is no mathematical objective for Hierarchical clustering.
 -All the approaches to calculate the similarity between clusters has its own disadvantages.
 -High space and time complexity for Hierarchical clustering. Hence this clustering algorithm cannot be used when we have huge data.


